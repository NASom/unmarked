<<echo=false>>=
options(width=70)
options(continue=" ")
@

\documentclass[a4paper]{article}
\usepackage[OT1]{fontenc}
\usepackage[nogin]{Sweave}
\usepackage{bm}
\usepackage{natbib}
\usepackage{fullpage}
\bibliographystyle{plain}

\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

%%\VignetteIndexEntry{Distsamp}

\title{Distance-sampling analysis in unmarked}
\author{Richard Chandler\\Department of Natural Resources Conservation\\%
  University of Massachusetts Amherst}


\begin{document}

\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}

\maketitle

\abstract{Distance-sampling is a wildlife sampling technique used to estimate population size or density. Describing how density varies spatially is often of equal interest; however, conventional methods of analysis do not allow for explicit modeling of both density and detection probability. The function \code{distsamp} implements the multinomial-Poisson mixture model of Royle et. al \citep{royle_distsamp_2004}, which was developed to overcome this limitation. This model requires that line or point transects are spatially replicated and that distance data are recorded in discrete intervals. This document describes how to format data, fit models, and manipulate results in package \package{unmarked}. It does not cover the statistical theory and assumptions underlying distance-sampling, which the user is expected to be familiar with. }


\section{Introduction}

In distance sampling, the study design (line- or point-transect), distance class break points, transect lengths, and units of measurement need to be accounted for in the analysis. Package \package{unmarked} uses an S4 class called \code{unmarkedFrameDS} to store data and metadata in a way that allows for easy data manipulation, summarization, and model specification.
                              
\section{Importing, formatting, and summarizing data}

The first step is to import the data into R. The simplest option is to use the \code{read.csv} function to import a .csv file that has been formatted so that each row represents a transect, and columns describe either the number of individuals detected in each distance interval or transect-specific covariates. Alternatively, if data were not recorded in discrete distance intervals, a .csv file could be imported that contains a row for each individual detected and columns for the distances and transect names. This could then be converted to transect-level data using the function \code{formatDistData}. For example, 

<<>>=
library(unmarked)
dists <- read.csv(system.file("csv", "distdata.csv", package="unmarked"))
head(dists, 10)
levels(dists$transect) <- c(levels(dists$transect), "g")
levels(dists$transect)
yDat <- formatDistData(dists, distCol="distance", 
	transectNameCol="transect", dist.breaks=c(0, 5, 10, 15, 20)) 
yDat
@

Here we have created an object called yDat that contains counts for each transect (row) in each distance interval (columns). Note the method used to include transect "g", which was surveyd but where no individuals were detected. It is important that all survyed transects are included in the analysis.

Suppose there also exists transect-specific covariate data. 

<<>>=
(covs <- data.frame(canopyHt = c(5, 8, 3, 2, 4, 7, 5), 
	habitat = c('A','A','A','A','B','B','B'), row.names=letters[1:7]))
@

These data can now be organized along with the associated metadata using the function \code{unmarkedFrameDS}. 


<<>>=
umf <- unmarkedFrameDS(y=as.matrix(yDat), siteCovs=covs, survey="line", 
	dist.breaks=c(0, 5, 10, 15, 20), tlength=rep(100, 7),
	unitsIn="m")	
@

Note that there is no \code{obsCovs} argument, meaning that distance-interval-level covariates cannot be included in the analysis. The call to \code{unmarkedFrameDS} indicates that the data were collected on seven line transects, each 100 meters long, and detections were tabulated into distance intervals defined by the \code{dist.breaks} cutpoints. It is important that both transect lengths and distance break points are provided in the same units specified by \code{unitsIn}.

We can look at these data, summarize them, and plot them usng a variety of methods.

<<umfhist,fig=TRUE,include=FALSE,width=4,height=4>>=
umf
summary(umf)
hist(umf, xlab="distance (m)", main="", cex.lab=0.8, cex.axis=0.8)
@

\begin{figure}[tb]
  \centering
  \includegraphics{distsamp-umfhist}
  \caption{Histogram of detection distances.}
  \label{fig:umfhist}
\end{figure}

\section{Model fitting}

Now that we have put our data into an object of class \code{unmarkedFrameDS}, we are ready to fit some models with \code{distsamp}. The first argument is a \code{formula} which specifies the detection covariates followed by the density (or abundance) covariates. The only other required argument is the \code{data}, but several other optional arguments exist. By default, the half-normal detection function is used to model density in animals / ha. The detection function can be selected using the \code{keyfun} argument. The response can be changed from "density", to "abund" with the \code{output} argument. Note, however, that when line transect lenghs differ, they must be accounted for so abundance is actually animals per unit transect length. When modeling density, the output units can be changed from "ha" to "kmsq" using the \code{unitsOut} argument. \code{distsamp} also includes the arguments \code{starts}, \code{method}, and \code{control}, which are common to all unmarked fitting functions.

Below are a series of models that demonstrate \code{distsamp}'s arguments and defaults.

<<>>=
hn_Null <- distsamp(~1~1, umf)
hn_Null <- distsamp(~1~1, umf, keyfun="halfnorm", output="density", 
	unitsOut="ha")
haz_Null <- distsamp(~1~1, umf, keyfun="hazard")
hn_Hab.Ht <- distsamp(~canopyHt ~habitat, umf)
@

The first two models are the same, a null half-normal detection function with density returned in animals / ha (on the log-scale). The third model uses the hazard-rate detection function, and the fourth model includes covariates affecting the Poisson mean ($\bm{\lambda}$) and the half-normal shape parameter ($\bm{\sigma}$).

Once a model has been fit, typing it's name will display parameter estimate information and AIC. A summary method shows extra details including the scale on which parameters were estimated and convergence results. 

<<>>=
summary(haz_Null)
@

Back-transforming estimates to the original scale and obtaining standard errors via the delta method is easily accomplished:

<<>>=
names(haz_Null)
backTransform(haz_Null, whichEstimate="state")
backTransform(haz_Null, whichEstimate="det")
backTransform(haz_Null, whichEstimate="scale")
backTransform(linearComb(hn_Hab.Ht['det'], c(0, 5)))
@

Note that when covariates are present, \code{backTransform} in conjunction with \code{linearComb} can be used for a single covariate combination. A \code{predict} method exits as well, which is especially useful when multiple covariate combinations are of interest. This method also facilitates plotting. Suppose we wanted model predictions from the covariate model along the range of covariate values studied. First we need make new data.frames holding the desired covariate combinations. Note that column names must match those of the original data, and factor variables must contain the same levels.


<<>>=
(habConstant <- data.frame(canopyHt = seq(2, 8, length=20), 
	habitat=factor("A", levels=c("A", "B"))))
(htConstant <- data.frame(canopyHt = 5, 
	habitat=factor(c("A", "B"))))
(Elambda <- predict(hn_Hab.Ht, type="state", newdata=htConstant, 
	appendData=TRUE))
head(Esigma <- predict(hn_Hab.Ht, type="det", newdata=habConstant, 
	appendData=TRUE))
@


Once predictions have been made for the new data, plotting is straight-forward. Figure x, shows density as a function of habitat type, and fig xb shows that $\bm{\sigma}$ is not related to canopy height.


<<fig=TRUE,width=6,height=3>>=
par(mfrow=c(1, 2))
with(Elambda, {
	x <- barplot(Predicted, names=habitat, xlab="Habitat", 
		ylab="Density (animals / ha)", ylim=c(0, 25), cex.names=0.7, 
        cex.lab=0.7, cex.axis=0.7)
	arrows(x, Predicted, x, Predicted+SE, code=3, angle=90, length=0.05)
	box()
	})
with(Esigma, {
	plot(canopyHt, Predicted, type="l", xlab="Canopy height", 
		ylab=expression(paste("Half-normal shape parameter (", sigma, ")", 
        sep="")), ylim=c(0, 20), cex=0.7, cex.lab=0.7,
    cex.axis=0.7)
	lines(canopyHt, Predicted+SE, lty=2)		
	lines(canopyHt, Predicted-SE, lty=2)
	})			
@



Plots of the detection function parameters can be less informative than plots of the detection functions themselves. To do the latter, we can plug predicted values of $\bm{\sigma}$ at given covariate values into the \code{gxhn} function. For instance, to plot the half-normal function at a canopy height of 2m, $\bm{\sigma}$ can be set to 10.8, the predicted value shown above. The available detection functions are described on the \code{detFuns} help page. Probability density functions such as \code{dxhn} can be plotted with the distance histogram, using a similar approach with the \code{hist} method for \code{unmarkedFitDS} objcets.


<<fig=TRUE,width=6,height=3>>=
par(mfrow=c(1, 2))
plot(function(x) gxhn(x, sigma=10.8), 0, 20, xlab="Distance (m)", 
	ylab="Detection probability at 2m canopy ht.", cex=0.7, cex.lab=0.7,
    cex.axis=0.7, las=1)
hist(hn_Null, xlab="Distance (m)", ylab="Probability density", main="", 
    ylim=c(0, 0.1), cex.lab=0.7, cex.axis=0.7, las=1)
plot(function(x) dxhn(x, sigma=8.7), 0, 20, add=TRUE, col="blue")
plot(function(x) dxhn(x, sigma=10.2), 0, 20, add=TRUE, col="green")
legend('topright', c("CanHt=2", "Null", "CanHt=8"), 
	col=c("blue", "black", "green"), lty=1, cex=0.5)
@


This document has emphasized methods tailored to distance-sampling analysis; however, it is important to realize that the more general methods available in package \package{unmarked} can be applied to distsamp related objects. For example, model selection and model fit evaluation can be accomplished using the same methods, \code{modSel} and \code{parboot}, common to all unmarked models.


\bibliography{unmarked}


\end{document}
